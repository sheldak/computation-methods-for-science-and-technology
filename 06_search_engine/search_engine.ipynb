{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse.linalg import svds\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_num = 2000\n",
    "\n",
    "def separate_text_file():\n",
    "    \"\"\" Separating text.txt file with many articles into separate articles\"\"\"\n",
    "    with open(\"application/text.txt\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "        # every article is starting from \"@@[number]\" so it is good to split text using \"@@\"\n",
    "        articles = text.split(\"@@\")\n",
    "\n",
    "        # to remove initial number in every article\n",
    "        numbers = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "\n",
    "        curr_article = 0\n",
    "\n",
    "        for inx in range(1, len(articles)):\n",
    "            if len(articles[inx]) > 972:    # to get exactly 2000 shortest articles from above 4000 in \"text.txt\"\n",
    "                continue\n",
    "\n",
    "            article_file = open(\"application/texts/\" + str(curr_article) + \".txt\", \"w\")\n",
    "\n",
    "            # sometimes there is \" @ @ @ @ \" in text so here is removed\n",
    "            articles[inx] = articles[inx].replace(\"@ \", \"\")\n",
    "\n",
    "            # every article starts with number, this loop is to remove this number\n",
    "            letter = 0\n",
    "            while articles[inx][letter] in numbers:\n",
    "                letter += 1\n",
    "\n",
    "            # writing article to the file\n",
    "            article_file.write(articles[inx][letter+1:])\n",
    "            article_file.close()\n",
    "\n",
    "            curr_article += 1\n",
    "\n",
    "\n",
    "def make_dictionary_and_matrix():\n",
    "    \"\"\" Making dictionary with all words in articles files in \"texts\" directory.\n",
    "        Key is a word and value is its index.\n",
    "\n",
    "        Then creating sparse term-by-document matrix.\n",
    "        Matrix's row represents a word and column represents an article.\n",
    "        matrix[i, j] = k means that there is k words having index i in dictionary in text \"texts/j.txt\"\n",
    "    \"\"\"\n",
    "\n",
    "    # result dictionary with all words in all articles\n",
    "    dictionary = {}\n",
    "\n",
    "    # to have an index of next word in dictionary\n",
    "    curr_word = 0\n",
    "\n",
    "    # stop words to remove them from dictionary\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # object which will stem the words\n",
    "    porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "    words_by_article = []\n",
    "    for i in range(articles_num):\n",
    "        with open(\"application/texts/\" + str(i) + \".txt\") as file:\n",
    "            # article text from file\n",
    "            text = file.read()\n",
    "\n",
    "            # text split on words using nltk library\n",
    "            word_tokens = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "            # filtering words to remove these which are \"stop words\"\n",
    "            filter_words = list(filter(lambda w: w not in stop_words, word_tokens))\n",
    "\n",
    "            # stemming words\n",
    "            stemmed_words = list(map(lambda w: porter_stemmer.stem(w), filter_words))\n",
    "\n",
    "            # writing new words to dictionary\n",
    "            for word in stemmed_words:\n",
    "                if word not in dictionary:\n",
    "                    dictionary[word] = curr_word\n",
    "                    curr_word += 1\n",
    "\n",
    "            # to write them later to the matrix\n",
    "            words_by_article.append(stemmed_words)\n",
    "\n",
    "    # making sparse matrix\n",
    "    matrix = scipy.sparse.lil_matrix((len(dictionary), articles_num), dtype=float)\n",
    "\n",
    "    # filling matrix\n",
    "    for i in range(articles_num):\n",
    "        for word in words_by_article[i]:\n",
    "            matrix[dictionary[word], i] += 1\n",
    "\n",
    "    # converting to csr matrix for faster operations\n",
    "    matrix = scipy.sparse.csr_matrix(matrix)\n",
    "\n",
    "    return matrix, dictionary\n",
    "\n",
    "\n",
    "def multiply_by_inverse_document_frequency(matrix):\n",
    "    \"\"\" Multiplying every row of matrix by its inverse document frequency. \"\"\"\n",
    "\n",
    "    matrix = scipy.sparse.csr_matrix.toarray(matrix)\n",
    "\n",
    "    for word_inx in range(matrix.shape[0]):\n",
    "        articles_with_word = len(matrix[word_inx].nonzero()[0])\n",
    "        idf = math.log(articles_num / articles_with_word)\n",
    "        matrix[word_inx] *= idf\n",
    "\n",
    "    return scipy.sparse.csr_matrix(matrix)\n",
    "\n",
    "\n",
    "def remove_noise(matrix, k):\n",
    "    \"\"\" Removing noise by singular value decomposition and low rank approximation. \"\"\"\n",
    "    matrix = scipy.sparse.csc_matrix(matrix)\n",
    "    u, s, vt = svds(matrix, k)\n",
    "\n",
    "    # it will be numpy array\n",
    "    new_matrix = u @ np.diag(s) @ vt\n",
    "\n",
    "    return new_matrix\n",
    "\n",
    "\n",
    "def get_matrix_and_dictionary(use_idf=False, reduce_noise=False, k=1):\n",
    "    \"\"\" Getting term-by-document matrix for searches and dictionary with all words.\n",
    "\n",
    "    Parameters:\n",
    "    use_idf - whether multiply the matrix by inverse document frequency\n",
    "    use_approximation - if reduce noise by singular value decomposition\n",
    "                        and low rank approximation\n",
    "    k - number of singular values for low rank approximation\n",
    "    \"\"\"\n",
    "\n",
    "    matrix, dictionary = make_dictionary_and_matrix()\n",
    "\n",
    "    if use_idf:\n",
    "        matrix = multiply_by_inverse_document_frequency(matrix)\n",
    "    else:\n",
    "        matrix = scipy.sparse.csr_matrix.toarray(matrix)\n",
    "\n",
    "    if reduce_noise:\n",
    "        matrix = remove_noise(matrix, k)\n",
    "    elif type(matrix) is not np.ndarray:\n",
    "        matrix = scipy.sparse.csr_matrix.toarray(matrix)\n",
    "\n",
    "    # normalization using numpy array\n",
    "    for column in range(matrix.shape[1]):\n",
    "        norm = np.linalg.norm(matrix[:, column])\n",
    "        if norm != 0:\n",
    "            matrix[:, column] /= norm\n",
    "\n",
    "    # converting to sparse matrix for future operations\n",
    "    matrix = scipy.sparse.csc_matrix(matrix)\n",
    "\n",
    "    return matrix, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vector(dictionary, text):\n",
    "    \"\"\" Creating a bag-of-words for text to search it. \"\"\"\n",
    "\n",
    "    # stop words to remove them from the text to search\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # object which will stem the words\n",
    "    porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "    # text split on words using nltk library\n",
    "    word_tokens = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    # filtering words to remove these which are \"stop words\"\n",
    "    filter_words = list(filter(lambda w: w not in stop_words, word_tokens))\n",
    "\n",
    "    # stemming words\n",
    "    stemmed_words = list(map(lambda w: porter_stemmer.stem(w), filter_words))\n",
    "\n",
    "    # bag-of-words vector of the text to search\n",
    "    text_vector = scipy.sparse.lil_matrix((len(dictionary), 1), dtype=float)\n",
    "\n",
    "    # adding words occurrence to the vector\n",
    "    for word in stemmed_words:\n",
    "        if word in dictionary:\n",
    "            text_vector[dictionary[word], 0] += 1\n",
    "\n",
    "    # converting to csr matrix for faster operations\n",
    "    text_vector = scipy.sparse.csc_matrix(text_vector)\n",
    "\n",
    "    # normalization\n",
    "    text_vector /= scipy.sparse.linalg.norm(text_vector)\n",
    "\n",
    "    return text_vector\n",
    "\n",
    "\n",
    "def search(dictionary, matrix, text, k):\n",
    "    \"\"\" Searching word and returning k texts with the highest probability to contain that text. \"\"\"\n",
    "\n",
    "    # getting bag-of-words vector for the text to search\n",
    "    text_vector = get_text_vector(dictionary, text)\n",
    "\n",
    "    # getting correlation for every article\n",
    "    correlation = []\n",
    "    for i in range(matrix.shape[1]):\n",
    "        word_correlation = (text_vector[:, 0].T @ matrix[:, i])[0, 0]\n",
    "\n",
    "        correlation.append((word_correlation, i))\n",
    "\n",
    "    # sorting to get the most suitable articles\n",
    "    correlation.sort(reverse=True)\n",
    "\n",
    "    # printing result\n",
    "    for i in range(k):\n",
    "        print(f\"Article: texts/{correlation[i][1]}.txt, correlation {correlation[i][0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_1, _ = get_matrix_and_dictionary(use_idf=False, reduce_noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_2, _ = get_matrix_and_dictionary(use_idf=True, reduce_noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes ~10s to run\n",
    "matrix_3, _ = get_matrix_and_dictionary(use_idf=True, reduce_noise=True, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes ~10s to run\n",
    "matrix_4, _ = get_matrix_and_dictionary(use_idf=True, reduce_noise=True, k=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes ~30s to run\n",
    "matrix_5, dictionary = get_matrix_and_dictionary(use_idf=True, reduce_noise=True, k=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First text to search\n",
    "Below is a text from file \"texts/1.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"becomes King of Assyria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/1.txt, correlation 0.35043832202523123\n",
      "Article: texts/20.txt, correlation 0.3198010745334157\n",
      "Article: texts/58.txt, correlation 0.18461625854057045\n",
      "Article: texts/465.txt, correlation 0.1825741858350554\n",
      "Article: texts/22.txt, correlation 0.1404878717372541\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_1, text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/1.txt, correlation 0.42601397501526733\n",
      "Article: texts/20.txt, correlation 0.2923672825251265\n",
      "Article: texts/58.txt, correlation 0.28484343132098255\n",
      "Article: texts/465.txt, correlation 0.14441769309426203\n",
      "Article: texts/22.txt, correlation 0.13167807681306165\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_2, text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/20.txt, correlation 0.19637239726209893\n",
      "Article: texts/58.txt, correlation 0.19509811414321251\n",
      "Article: texts/1487.txt, correlation 0.18868143146140798\n",
      "Article: texts/604.txt, correlation 0.18784921718156544\n",
      "Article: texts/581.txt, correlation 0.18784548114234678\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_3, text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/1.txt, correlation 0.3798343225071507\n",
      "Article: texts/20.txt, correlation 0.3670825688422151\n",
      "Article: texts/22.txt, correlation 0.31005959610080097\n",
      "Article: texts/465.txt, correlation 0.2914002772705761\n",
      "Article: texts/58.txt, correlation 0.2764393679409319\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_4, text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/1.txt, correlation 0.4176139102982707\n",
      "Article: texts/20.txt, correlation 0.3079106724704089\n",
      "Article: texts/465.txt, correlation 0.3060938859713124\n",
      "Article: texts/58.txt, correlation 0.28335615105823503\n",
      "Article: texts/793.txt, correlation 0.1542262083612811\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_5, text, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second text to search\n",
    "Below is a text from file \"texts/0.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"higher values mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/0.txt, correlation 0.09141414530040079\n",
      "Article: texts/1885.txt, correlation 0.0912870929175277\n",
      "Article: texts/187.txt, correlation 0.08006407690254358\n",
      "Article: texts/602.txt, correlation 0.06666666666666667\n",
      "Article: texts/571.txt, correlation 0.06225728063646904\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_1, text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/1885.txt, correlation 0.10626668533334137\n",
      "Article: texts/0.txt, correlation 0.10244915526154882\n",
      "Article: texts/187.txt, correlation 0.08289186061947922\n",
      "Article: texts/602.txt, correlation 0.06822502392658282\n",
      "Article: texts/62.txt, correlation 0.06652606205783809\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_2, text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/0.txt, correlation 0.029587940330816215\n",
      "Article: texts/220.txt, correlation 0.02653844362333795\n",
      "Article: texts/713.txt, correlation 0.024265018356019186\n",
      "Article: texts/452.txt, correlation 0.02388917125647607\n",
      "Article: texts/704.txt, correlation 0.02326578412824576\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_3, text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/0.txt, correlation 0.10333551373910102\n",
      "Article: texts/305.txt, correlation 0.047408418377526435\n",
      "Article: texts/742.txt, correlation 0.0429915991243701\n",
      "Article: texts/575.txt, correlation 0.04027561526219685\n",
      "Article: texts/550.txt, correlation 0.039880665861355906\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_4, text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/0.txt, correlation 0.10291575489625168\n",
      "Article: texts/1563.txt, correlation 0.06441609935990149\n",
      "Article: texts/187.txt, correlation 0.06257364968453359\n",
      "Article: texts/1455.txt, correlation 0.059649529580734204\n",
      "Article: texts/62.txt, correlation 0.05778488748151628\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_5, text, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third text to search\n",
    "Below is also a text from file \"texts/0.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"other factors are the same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/212.txt, correlation 0.06900655593423542\n",
      "Article: texts/186.txt, correlation 0.06551217820804184\n",
      "Article: texts/0.txt, correlation 0.05277798139692595\n",
      "Article: texts/459.txt, correlation 0.04612656040144425\n",
      "Article: texts/1091.txt, correlation 0.04339630366027462\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_1, text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/212.txt, correlation 0.12180166260497147\n",
      "Article: texts/186.txt, correlation 0.11181890537025016\n",
      "Article: texts/1091.txt, correlation 0.08867986475618805\n",
      "Article: texts/459.txt, correlation 0.08352825451532035\n",
      "Article: texts/0.txt, correlation 0.06217175333778234\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_2, text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/427.txt, correlation 0.04322494531915992\n",
      "Article: texts/1509.txt, correlation 0.01790222565458324\n",
      "Article: texts/757.txt, correlation 0.015465973585733666\n",
      "Article: texts/0.txt, correlation 0.01482603454055907\n",
      "Article: texts/901.txt, correlation 0.01455420094866487\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_3, text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/0.txt, correlation 0.061466874995638514\n",
      "Article: texts/459.txt, correlation 0.045212107250326365\n",
      "Article: texts/427.txt, correlation 0.0441921417571892\n",
      "Article: texts/1091.txt, correlation 0.04191379035935991\n",
      "Article: texts/1016.txt, correlation 0.03719579183736633\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_4, text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: texts/186.txt, correlation 0.10352905236651629\n",
      "Article: texts/459.txt, correlation 0.08175068707509976\n",
      "Article: texts/1091.txt, correlation 0.07899041060576374\n",
      "Article: texts/212.txt, correlation 0.07319548708601338\n",
      "Article: texts/0.txt, correlation 0.062318030717708646\n"
     ]
    }
   ],
   "source": [
    "search(dictionary, matrix_5, text, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
